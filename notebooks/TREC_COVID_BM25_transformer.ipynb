{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TREC_COVID_BM25_transformer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huertas97/TREC_COVID_sentence_transformers/blob/main/notebooks/TREC_COVID_BM25_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7qk7hXNYwED"
      },
      "source": [
        "# Introduction\n",
        "This notebook shows a full pipeline for evaluating a Transformer-based model on TREC-COVID round 1. \n",
        "\n",
        "Scores from BM25 Okapi algorithm and Sentence Transformers embeddings are computed separately. Thus, the TREC-COVID metrics can also be computed for each strategy separately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C25pPJNnk7t",
        "outputId": "b42d51da-3ab6-473d-f591-98bdad954242"
      },
      "source": [
        "!pip install -U -q sentence-transformers\n",
        "!pip install -q scispacy\n",
        "!pip install -q https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz\n",
        "!pip install -U -q tqdm\n",
        "!pip install rank-bm25\n",
        "# tqdm._instances.clear()\n",
        "# from importlib import reload\n",
        "# logging.shutdown()\n",
        "# reload(logging)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 10.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 29.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 40.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 54.9MB/s \n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.4MB 8.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 13.0MB 32.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 53.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 33.1MB 90kB/s \n",
            "\u001b[?25h  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hCollecting rank-bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank-bm25) (1.18.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEgEV0Ou4ZXN"
      },
      "source": [
        "# Clone the github repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlIPqy204bX4",
        "outputId": "29569dd1-3632-42c3-bb00-8746aaa36e75"
      },
      "source": [
        "!git clone https://github.com/Huertas97/TREC_COVID_sentence_transformers.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TREC_COVID_sentence_transformers'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 38 (delta 16), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 10.57 MiB | 30.56 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLuE2h2s4mAA",
        "outputId": "6741061b-03e5-4085-f7f9-b77dff2fee1a"
      },
      "source": [
        "%cd TREC_COVID_sentence_transformers/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TREC_COVID_sentence_transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtHYUTV8ZMZ2"
      },
      "source": [
        "# Download TREC-COVID DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziIAyV9FXhjZ",
        "outputId": "f89f41ef-d8aa-45ea-e397-b58a3f2cff2f"
      },
      "source": [
        "# Script to download and preprocess CORD-19 documents for TREC-COVID task\n",
        "!python ./scripts/build_trec_covid_data.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 11:03:00.095375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-12-12 11:03:02 - ------ Downloading docids-rnd1.txt ------\n",
            "460kB [00:00, 1.07MB/s]\n",
            "2020-12-12 11:03:03 - ------ Downloading topics-rnd1.xml ------\n",
            "10.3kB [00:00, 6.80MB/s]       \n",
            "2020-12-12 11:03:03 - ------ Downloading qrels-rnd1.txt ------\n",
            "150kB [00:00, 782kB/s] \n",
            "2020-12-12 11:03:04 - ------ Downloading cord-19_2020-04-10.tar.gz ------\n",
            "100% 1.52G/1.52G [01:40<00:00, 15.1MB/s]\n",
            "2020-12-12 11:04:45 - ------ Uncompressing cord-19_2020-04-10.tar.gz ------\n",
            "1.52GB [02:07, 11.9MB/s]                \n",
            "2020-12-12 11:06:52 - ------ Uncompressing ./2020-04-10/noncomm_use_subset.tar.gz ------\n",
            "76.4MB [00:03, 20.3MB/s]                \n",
            "2020-12-12 11:06:56 - ------ Uncompressing ./2020-04-10/custom_license.tar.gz ------\n",
            "660MB [01:22, 8.03MB/s]               \n",
            "2020-12-12 11:08:18 - ------ Uncompressing ./2020-04-10/comm_use_subset.tar.gz ------\n",
            "367MB [01:01, 5.92MB/s]               \n",
            "2020-12-12 11:09:20 - ------ Uncompressing ./2020-04-10/biorxiv_medrxiv.tar.gz ------\n",
            "22.4MB [00:01, 11.8MB/s]                \n",
            "CORD documents extracted: 100% 5553/5553 [00:19<00:00, 285.77it/s]\n",
            "2020-12-12 11:09:47 - ------ Saving parsed CORD-19 documents ------\n",
            "2020-12-12 11:09:48 - Saved in trec_covid_data/df_docs.pkl\n",
            "2020-12-12 11:09:48 - ------ Finished ------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPmvDz4mdNYv"
      },
      "source": [
        "# BM25 Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnpCCdP9JPui",
        "outputId": "c295198d-b1b9-4d10-bf24-7eb69c91ddc0"
      },
      "source": [
        "# Script to compute BM25 Okapi scaled scores\r\n",
        "!python ./scripts/bm25_trec_covid.py -a -t -f  --data ./trec_covid_data/df_docs.pkl"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------- Loading scispacy en_core_sci_sm model --------\n",
            "-------- Building corpus --------\n",
            "-------- Extracting topics --------\n",
            "Corpus:   0% 0/3 [00:00<?, ?it/s]-------- Adding fulltext corpus to BM25 --------\n",
            "Tokenized: 100% 5553/5553 [01:25<00:00, 65.24it/s]\n",
            "-------- fulltext: BM25 scores for each topic --------\n",
            "Topic: 100% 30/30 [00:01<00:00, 15.76it/s]\n",
            "Corpus:  33% 1/3 [01:29<02:59, 89.95s/it]-------- Adding abstract corpus to BM25 --------\n",
            "Tokenized: 100% 5553/5553 [00:05<00:00, 952.16it/s] \n",
            "-------- abstract: BM25 scores for each topic --------\n",
            "Topic: 100% 30/30 [00:01<00:00, 20.01it/s]\n",
            "Corpus:  67% 2/3 [01:37<01:05, 65.33s/it]-------- Adding title corpus to BM25 --------\n",
            "Tokenized: 100% 5553/5553 [00:00<00:00, 7511.97it/s]\n",
            "-------- title: BM25 scores for each topic --------\n",
            "Topic: 100% 30/30 [00:01<00:00, 26.14it/s]\n",
            "Corpus: 100% 3/3 [01:39<00:00, 33.27s/it]\n",
            "2020-12-12 11:11:38 - -------- Summation BM25 scores for all corpus --------\n",
            "2020-12-12 11:11:38 - -------- Scaling scores (max score = 9) --------\n",
            "2020-12-12 11:11:38 - -------- Saving BM25 results in ./results/df_BM25_sc.pkl --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5Un2d-UauhN"
      },
      "source": [
        "Now we are going to create a rank score for the document with BM25 and sentence embeddings models between:\n",
        "\n",
        "* query vs title\n",
        "* query vs abstract\n",
        "* query vs fulltext\n",
        "\n",
        "<br>\n",
        "\n",
        "* question vs title\n",
        "* question vs abstract\n",
        "* question vs fulltext\n",
        "\n",
        "<br>\n",
        "\n",
        "* narrative vs title\n",
        "* narrative vs abstract\n",
        "* narrative vs fulltext\n",
        "\n",
        "As the corpus is the most computational consuming we will create a corpus embeddings of title with all the queries, then the abstract and finall fulltext. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVQKJtslae7f"
      },
      "source": [
        "We are gonna try to compute an embedding of an abstract by split it in sentences and computing an embedding for each one. The bastract embedding will be the average of the sentence embeddings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtfXIl5XjJsB"
      },
      "source": [
        "#Cosine similarity \r\n",
        "\r\n",
        "Three scripts are availabe. Which one to use depends upon the model you want to use for computing embeddings (then used for calculating cosine similarity score). \r\n",
        "\r\n",
        "* If you want to use a single model from [Hugging Face](https://huggingface.co/models) or [Sentence Transformer](https://www.sbert.net/index.html) use `cos_sim_trec_covid.py`\r\n",
        "\r\n",
        "* If you want to apply an ensemble of models described aboved use `ensemble_cos_sim_trec_covid.py`\r\n",
        "\r\n",
        "* Finally, an ensemble of models applying a PCA is available with the script `ensemble_dim_red_cos_sim_trec_covid.py`. Pre-computed PCA are only avialble for multilingual models from Sentence Transformers:\r\n",
        "\r\n",
        "  * distiluse-base-multilingual-cased\r\n",
        "  * xlm-r-distilroberta-base-paraphrase-v1\r\n",
        "  * xlm-r-bert-base-nli-stsb-mean-tokens\r\n",
        "  * LaBSE\r\n",
        "  * distilbert-multilingual-nli-stsb-quora-ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjpQ3cpmkWXp"
      },
      "source": [
        "## Hugging Face model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTzKixBUjOBb"
      },
      "source": [
        "# Script to compute cosine similarity scores for clinicalcovid-bert-nli from Huggingface\r\n",
        "!python ./scripts/cos_sim_trec_covid.py -b 1000 -t -a -f \\\\\r\n",
        "--data ./trec_covid_data/df_docs.pkl --model manueltonneau/clinicalcovid-bert-nli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dla1e273UQz0"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY-69TlVU63r"
      },
      "source": [
        "# Ensemble 5 models \r\n",
        "!python ./scripts/ensemble_cos_sim_trec_covid.py -b 1000 \\\\\r\n",
        "-t -a --data ./trec_covid_data/df_docs.pkl \\\\\r\n",
        "--model  distiluse-base-multilingual-cased,xlm-r-distilroberta-base-paraphrase-v1,xlm-r-bert-base-nli-stsb-mean-tokens,LaBSE,distilbert-multilingual-nli-stsb-quora-ranking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4VQJVtv_yuB"
      },
      "source": [
        "## Ensemble and PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsuKtduSfvkN",
        "outputId": "299513d2-80f3-4c04-bc0d-e0cd03c00c06"
      },
      "source": [
        "# Help documentation\r\n",
        "!python ./scripts/ensemble_dim_red_cos_sim_trec_covid.py --help"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 11:35:16.098580: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\n",
            "Usage:\n",
            "\n",
            "    python ensemble_dim_red_cos_sim_trec_covid.py [options] \n",
            "\n",
            "Options:\n",
            "    -d, --data              Path to TREC-COVID parsed data\n",
            "    -p, --pca               Path to dataframe with model names and PCAs\n",
            "    -m, --model             Name of Transformer-based model from https://huggingface.co/pricing\n",
            "    -f, --fulltext          Bool: Include fulltext corpus for BM25 scoring\n",
            "    -a, --abstract          Bool: Include abstract corpus for BM25 scoring  \n",
            "    -t, --title             Bool: Include titles corpus for BM25 scoring  \n",
            "    -b, --batch             Batch size\n",
            "    -h, --help              Help documentation\n",
            "\n",
            "Example:\n",
            "    python ./scripts/ensemble_dim_red_cos_sim_trec_covid.py -b 1000 -t -a --data ./trec_covid_data/df_docs.pkl --model distiluse-base-multilingual-cased,distilbert-multilingual-nli-stsb-quora-ranking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BZYmx1D_0s4",
        "outputId": "d61842ff-d314-4019-ce70-0c4cde6ffe0e"
      },
      "source": [
        "# Ensemble 5 models with PCA\r\n",
        "!python ./scripts/ensemble_dim_red_cos_sim_trec_covid.py -b 1000 -t -a \\\\\r\n",
        "--data ./trec_covid_data/df_docs.pkl \\\\\r\n",
        "--pca ./PCA/df_multi_selected_99.pkl \\\\\r\n",
        "--model distiluse-base-multilingual-cased,xlm-r-distilroberta-base-paraphrase-v1,xlm-r-bert-base-nli-stsb-mean-tokens,LaBSE,distilbert-multilingual-nli-stsb-quora-ranking"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 07:59:56.436379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "-------- Loading scispacy en_core_sci_sm model --------\n",
            "2020-12-12 08:00:04 - -------- Loading SentenceTransformer model --------\n",
            "2020-12-12 08:00:04 - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
            "2020-12-12 08:00:04 - Did not find folder distiluse-base-multilingual-cased\n",
            "2020-12-12 08:00:04 - Try to download model from server: https://sbert.net/models/distiluse-base-multilingual-cased.zip\n",
            "2020-12-12 08:00:04 - Downloading sentence transformer model from https://sbert.net/models/distiluse-base-multilingual-cased.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_distiluse-base-multilingual-cased\n",
            "100% 504M/504M [00:17<00:00, 28.5MB/s]\n",
            "2020-12-12 08:00:27 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_distiluse-base-multilingual-cased\n",
            "2020-12-12 08:00:30 - Use pytorch device: cuda\n",
            "2020-12-12 08:00:30 - Load pretrained SentenceTransformer: xlm-r-distilroberta-base-paraphrase-v1\n",
            "2020-12-12 08:00:30 - Did not find folder xlm-r-distilroberta-base-paraphrase-v1\n",
            "2020-12-12 08:00:30 - Try to download model from server: https://sbert.net/models/xlm-r-distilroberta-base-paraphrase-v1.zip\n",
            "2020-12-12 08:00:30 - Downloading sentence transformer model from https://sbert.net/models/xlm-r-distilroberta-base-paraphrase-v1.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_xlm-r-distilroberta-base-paraphrase-v1\n",
            "100% 1.01G/1.01G [00:33<00:00, 30.3MB/s]\n",
            "2020-12-12 08:01:14 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_xlm-r-distilroberta-base-paraphrase-v1\n",
            "2020-12-12 08:01:20 - Use pytorch device: cuda\n",
            "2020-12-12 08:01:20 - Load pretrained SentenceTransformer: xlm-r-bert-base-nli-stsb-mean-tokens\n",
            "2020-12-12 08:01:20 - Did not find folder xlm-r-bert-base-nli-stsb-mean-tokens\n",
            "2020-12-12 08:01:20 - Try to download model from server: https://sbert.net/models/xlm-r-bert-base-nli-stsb-mean-tokens.zip\n",
            "2020-12-12 08:01:20 - Downloading sentence transformer model from https://sbert.net/models/xlm-r-bert-base-nli-stsb-mean-tokens.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_xlm-r-bert-base-nli-stsb-mean-tokens\n",
            "100% 1.01G/1.01G [00:33<00:00, 30.7MB/s]\n",
            "2020-12-12 08:02:03 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_xlm-r-bert-base-nli-stsb-mean-tokens\n",
            "2020-12-12 08:02:10 - Use pytorch device: cuda\n",
            "2020-12-12 08:02:10 - Load pretrained SentenceTransformer: LaBSE\n",
            "2020-12-12 08:02:10 - Did not find folder LaBSE\n",
            "2020-12-12 08:02:10 - Try to download model from server: https://sbert.net/models/LaBSE.zip\n",
            "2020-12-12 08:02:10 - Downloading sentence transformer model from https://sbert.net/models/LaBSE.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_LaBSE\n",
            "100% 1.75G/1.75G [00:55<00:00, 31.4MB/s]\n",
            "2020-12-12 08:03:30 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_LaBSE\n",
            "tcmalloc: large alloc 1539547136 bytes == 0x14ed38000 @  0x7fd3e91afb6b 0x7fd3e91cf379 0x7fd39a11c74e 0x7fd39a11e7b6 0x7fd3d4572fa2 0x7fd3d485dbd3 0x7fd3d4835207 0x7fd3d48502dc 0x7fd3d482c78a 0x7fd3d4835207 0x7fd3d48502dc 0x7fd3d491c0dd 0x7fd3e4b41a8a 0x7fd3e4b43681 0x7fd3e4940d1a 0x551555 0x5a9dac 0x50a433 0x50beb4 0x507be4 0x508ec2 0x594a01 0x549e8f 0x5515c1 0x5a9dac 0x50a433 0x50cc96 0x507be4 0x508ec2 0x594a01 0x549e8f\n",
            "tcmalloc: large alloc 1539547136 bytes == 0x7fd279c94000 @  0x7fd3e91afb6b 0x7fd3e91cf379 0x7fd39a11c74e 0x7fd39a11e7b6 0x7fd3d52b799d 0x7fd3e497a8d0 0x7fd3e45f8c1a 0x566bbc 0x50a433 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x508ec2 0x5a4c61 0x5a4fb8 0x4e012e 0x50a461 0x50beb4 0x507be4 0x588e5c 0x59fd0e 0x50d256 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x508ec2\n",
            "2020-12-12 08:04:00 - Use pytorch device: cuda\n",
            "2020-12-12 08:04:00 - Load pretrained SentenceTransformer: distilbert-multilingual-nli-stsb-quora-ranking\n",
            "2020-12-12 08:04:00 - Did not find folder distilbert-multilingual-nli-stsb-quora-ranking\n",
            "2020-12-12 08:04:00 - Try to download model from server: https://sbert.net/models/distilbert-multilingual-nli-stsb-quora-ranking.zip\n",
            "2020-12-12 08:04:00 - Downloading sentence transformer model from https://sbert.net/models/distilbert-multilingual-nli-stsb-quora-ranking.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_distilbert-multilingual-nli-stsb-quora-ranking\n",
            "100% 501M/501M [00:18<00:00, 26.9MB/s]\n",
            "2020-12-12 08:04:24 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_distilbert-multilingual-nli-stsb-quora-ranking\n",
            "2020-12-12 08:04:27 - Use pytorch device: cuda\n",
            "Embedding dimensions: 685\n",
            "2020-12-12 08:04:27 - -------- Building corpus --------\n",
            "2020-12-12 08:04:27 - -------- Extracting topics --------\n",
            "Batches:   0% 0/6 [00:00<?, ?it/s]-------- Computing scores --------\n",
            "Batches:  17% 1/6 [05:13<26:07, 313.60s/it]-------- Computing scores --------\n",
            "Batches:  33% 2/6 [09:38<19:55, 298.90s/it]-------- Computing scores --------\n",
            "Batches:  50% 3/6 [14:57<15:14, 304.97s/it]-------- Computing scores --------\n",
            "Batches:  67% 4/6 [20:36<10:30, 315.13s/it]-------- Computing scores --------\n",
            "Batches:  83% 5/6 [25:42<05:12, 312.60s/it]-------- Computing scores --------\n",
            "Batches: 100% 6/6 [28:35<00:00, 285.88s/it]\n",
            "2020-12-12 08:33:02 - -------- Summation cosine similairty scores for all batches --------\n",
            "2020-12-12 08:33:02 - -------- Summation cosine similairty scores for all corpus --------\n",
            "2020-12-12 08:33:02 - -------- Saving results in ./results/df_cos_sim_sc_distiluse-base-multilingual-cased_xlm-r-distilroberta-base-paraphrase-v1_xlm-r-bert-base-nli-stsb-mean-tokens_LaBSE_distilbert-multilingual-nli-stsb-quora-ranking.pkl --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcgUUntSBVzl",
        "outputId": "f08d93cc-e888-4036-ba19-2466a4afdc5d"
      },
      "source": [
        "# Ensemble 2 best multilingual models from STSb\r\n",
        "!python ./scripts/ensemble_dim_red_cos_sim_trec_covid.py -b 1000 -t -a \\\\\r\n",
        "--data ./trec_covid_data/df_docs.pkl \\\\\r\n",
        "--pca ./PCA/df_multi_selected_99.pkl \\\\\r\n",
        "--model xlm-r-distilroberta-base-paraphrase-v1,xlm-r-bert-base-nli-stsb-mean-tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 08:33:29.989419: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "-------- Loading scispacy en_core_sci_sm model --------\n",
            "2020-12-12 08:33:43 - -------- Loading SentenceTransformer model --------\n",
            "2020-12-12 08:33:43 - Load pretrained SentenceTransformer: xlm-r-distilroberta-base-paraphrase-v1\n",
            "2020-12-12 08:33:43 - Did not find folder xlm-r-distilroberta-base-paraphrase-v1\n",
            "2020-12-12 08:33:43 - Try to download model from server: https://sbert.net/models/xlm-r-distilroberta-base-paraphrase-v1.zip\n",
            "2020-12-12 08:33:43 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_xlm-r-distilroberta-base-paraphrase-v1\n",
            "2020-12-12 08:34:20 - Use pytorch device: cuda\n",
            "2020-12-12 08:34:20 - Load pretrained SentenceTransformer: xlm-r-bert-base-nli-stsb-mean-tokens\n",
            "2020-12-12 08:34:20 - Did not find folder xlm-r-bert-base-nli-stsb-mean-tokens\n",
            "2020-12-12 08:34:20 - Try to download model from server: https://sbert.net/models/xlm-r-bert-base-nli-stsb-mean-tokens.zip\n",
            "2020-12-12 08:34:20 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_xlm-r-bert-base-nli-stsb-mean-tokens\n",
            "2020-12-12 08:34:57 - Use pytorch device: cuda\n",
            "Embedding dimensions: 193\n",
            "2020-12-12 08:34:57 - -------- Building corpus --------\n",
            "2020-12-12 08:34:57 - -------- Extracting topics --------\n",
            "Batches:   0% 0/6 [00:00<?, ?it/s]-------- Computing scores --------\n",
            "Batches:  17% 1/6 [02:42<13:31, 162.38s/it]-------- Computing scores --------\n",
            "Batches:  33% 2/6 [04:58<10:18, 154.64s/it]-------- Computing scores --------\n",
            "Batches:  50% 3/6 [07:46<07:55, 158.65s/it]-------- Computing scores --------\n",
            "Batches:  67% 4/6 [10:39<05:25, 162.72s/it]-------- Computing scores --------\n",
            "Batches:  83% 5/6 [13:13<02:40, 160.22s/it]-------- Computing scores --------\n",
            "Batches: 100% 6/6 [14:40<00:00, 146.76s/it]\n",
            "2020-12-12 08:49:38 - -------- Summation cosine similairty scores for all batches --------\n",
            "2020-12-12 08:49:38 - -------- Summation cosine similairty scores for all corpus --------\n",
            "2020-12-12 08:49:38 - -------- Saving results in ./results/df_cos_sim_sc_xlm-r-distilroberta-base-paraphrase-v1_xlm-r-bert-base-nli-stsb-mean-tokens.pkl --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVcJeOxqFak5",
        "outputId": "4ec0d69f-d479-4da3-edbd-5b7df9f5a556"
      },
      "source": [
        "# Ensemble of 3 best models in TREC-COVID\r\n",
        "!python ./scripts/ensemble_dim_red_cos_sim_trec_covid.py -b 1000 -t -a \\\\\r\n",
        "--data ./trec_covid_data/df_docs.pkl \\\\\r\n",
        "--pca ./PCA/df_multi_selected_99.pkl \\\\\r\n",
        "--model distiluse-base-multilingual-cased,LaBSE,distilbert-multilingual-nli-stsb-quora-ranking"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 11:11:40.660634: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "-------- Loading scispacy en_core_sci_sm model --------\n",
            "2020-12-12 11:11:48 - -------- Loading SentenceTransformer model --------\n",
            "2020-12-12 11:11:48 - Load pretrained SentenceTransformer: distiluse-base-multilingual-cased\n",
            "2020-12-12 11:11:48 - Did not find folder distiluse-base-multilingual-cased\n",
            "2020-12-12 11:11:48 - Try to download model from server: https://sbert.net/models/distiluse-base-multilingual-cased.zip\n",
            "2020-12-12 11:11:48 - Downloading sentence transformer model from https://sbert.net/models/distiluse-base-multilingual-cased.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_distiluse-base-multilingual-cased\n",
            "100% 504M/504M [00:06<00:00, 75.4MB/s]\n",
            "2020-12-12 11:12:00 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_distiluse-base-multilingual-cased\n",
            "2020-12-12 11:12:03 - Use pytorch device: cuda\n",
            "2020-12-12 11:12:03 - Load pretrained SentenceTransformer: LaBSE\n",
            "2020-12-12 11:12:03 - Did not find folder LaBSE\n",
            "2020-12-12 11:12:03 - Try to download model from server: https://sbert.net/models/LaBSE.zip\n",
            "2020-12-12 11:12:03 - Downloading sentence transformer model from https://sbert.net/models/LaBSE.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_LaBSE\n",
            "100% 1.75G/1.75G [00:25<00:00, 67.5MB/s]\n",
            "2020-12-12 11:13:16 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_LaBSE\n",
            "tcmalloc: large alloc 1539547136 bytes == 0x60ab0000 @  0x7f31a450db6b 0x7f31a452d379 0x7f315547a74e 0x7f315547c7b6 0x7f318f8d0fa2 0x7f318fbbbbd3 0x7f318fb93207 0x7f318fbae2dc 0x7f318fb8a78a 0x7f318fb93207 0x7f318fbae2dc 0x7f318fc7a0dd 0x7f319fe9fa8a 0x7f319fea1681 0x7f319fc9ed1a 0x551555 0x5a9dac 0x50a433 0x50beb4 0x507be4 0x508ec2 0x594a01 0x549e8f 0x5515c1 0x5a9dac 0x50a433 0x50cc96 0x507be4 0x508ec2 0x594a01 0x549e8f\n",
            "tcmalloc: large alloc 1539547136 bytes == 0xbc6ea000 @  0x7f31a450db6b 0x7f31a452d379 0x7f315547a74e 0x7f315547c7b6 0x7f319061599d 0x7f319fcd88d0 0x7f319f956c1a 0x566bbc 0x50a433 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x508ec2 0x5a4c61 0x5a4fb8 0x4e012e 0x50a461 0x50beb4 0x507be4 0x588e5c 0x59fd0e 0x50d256 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x508ec2\n",
            "2020-12-12 11:13:28 - Use pytorch device: cuda\n",
            "2020-12-12 11:13:28 - Load pretrained SentenceTransformer: distilbert-multilingual-nli-stsb-quora-ranking\n",
            "2020-12-12 11:13:28 - Did not find folder distilbert-multilingual-nli-stsb-quora-ranking\n",
            "2020-12-12 11:13:28 - Try to download model from server: https://sbert.net/models/distilbert-multilingual-nli-stsb-quora-ranking.zip\n",
            "2020-12-12 11:13:28 - Downloading sentence transformer model from https://sbert.net/models/distilbert-multilingual-nli-stsb-quora-ranking.zip and saving it at /root/.cache/torch/sentence_transformers/sbert.net_models_distilbert-multilingual-nli-stsb-quora-ranking\n",
            "100% 501M/501M [00:07<00:00, 64.8MB/s]\n",
            "2020-12-12 11:13:43 - Load SentenceTransformer from folder: /root/.cache/torch/sentence_transformers/sbert.net_models_distilbert-multilingual-nli-stsb-quora-ranking\n",
            "2020-12-12 11:13:46 - Use pytorch device: cuda\n",
            "Embedding dimensions: 492\n",
            "2020-12-12 11:13:46 - -------- Building corpus --------\n",
            "2020-12-12 11:13:46 - -------- Extracting topics --------\n",
            "Batches:   0% 0/6 [00:00<?, ?it/s]-------- Computing scores --------\n",
            "Batches:  17% 1/6 [02:37<13:07, 157.56s/it]-------- Computing scores --------\n",
            "Batches:  33% 2/6 [04:41<09:49, 147.50s/it]-------- Computing scores --------\n",
            "Batches:  50% 3/6 [07:14<07:27, 149.03s/it]-------- Computing scores --------\n",
            "Batches:  67% 4/6 [09:53<05:04, 152.00s/it]-------- Computing scores --------\n",
            "Batches:  83% 5/6 [12:13<02:28, 148.57s/it]-------- Computing scores --------\n",
            "Batches: 100% 6/6 [13:31<00:00, 135.20s/it]\n",
            "2020-12-12 11:27:18 - -------- Summation cosine similairty scores for all batches --------\n",
            "2020-12-12 11:27:18 - -------- Summation cosine similairty scores for all corpus --------\n",
            "2020-12-12 11:27:18 - -------- Saving results in ./results/df_cos_sim_sc_distiluse-base-multilingual-cased_LaBSE_distilbert-multilingual-nli-stsb-quora-ranking.pkl --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRGdUoqaF9c8"
      },
      "source": [
        "# Top k results\r\n",
        "\r\n",
        "Remember to put a relevant output name to the file created with the topk results (`-o` option). This file will be used to evaluate TREC-COVID expert-judgment scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4JucC3JF_UO",
        "outputId": "1f292fef-49af-4436-81de-0293e79794c9"
      },
      "source": [
        "# Script to extract topk scores for each topic\r\n",
        "!python ./scripts/topk_trec_covid.py --data ./trec_covid_data/df_docs.pkl \\\\\r\n",
        "-p ./results -f df_BM25_sc.pkl,df_cos_sim_sc_distiluse-base-multilingual-cased_LaBSE_distilbert-multilingual-nli-stsb-quora-ranking.pkl \\\\\r\n",
        "-o ensemble_3_models"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-12 11:29:10 - -------- Retrieving top 1000 scores for each topic --------\n",
            "2020-12-12 11:29:10 - -------- Saving results in ./results/ensemble_3_models --------\n",
            "2020-12-12 11:29:10 - -------- Finished --------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9pnEgdEvAa-"
      },
      "source": [
        "# EVAL TREC-COVID\r\n",
        "\r\n",
        "Clone the official repository and change the `.txt` file name with the scores desired to evaluate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEhHWuQOvFiA",
        "outputId": "38896f59-2a06-4775-bb2d-919318d3db50"
      },
      "source": [
        "!git clone https://github.com/usnistgov/trec_eval.git\n",
        "%cd trec_eval/\n",
        "!make\n",
        "%cd .."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'trec_eval'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 763 (delta 5), reused 3 (delta 0), pack-reused 749\u001b[K\n",
            "Receiving objects: 100% (763/763), 679.52 KiB | 2.05 MiB/s, done.\n",
            "Resolving deltas: 100% (491/491), done.\n",
            "/content/TREC_COVID_sentence_transformers/trec_eval\n",
            "gcc -g -I.  -Wall -DVERSIONID=\\\"9.0.7\\\"  -o trec_eval trec_eval.c formats.c meas_init.c meas_acc.c meas_avg.c meas_print_single.c meas_print_final.c get_qrels.c get_trec_results.c get_prefs.c get_qrels_prefs.c get_qrels_jg.c form_res_rels.c form_res_rels_jg.c form_prefs_counts.c utility_pool.c get_zscores.c convert_zscores.c measures.c  m_map.c m_P.c m_num_q.c m_num_ret.c m_num_rel.c m_num_rel_ret.c m_gm_map.c m_Rprec.c m_recip_rank.c m_bpref.c m_iprec_at_recall.c m_recall.c m_Rprec_mult.c m_utility.c m_11pt_avg.c m_ndcg.c m_ndcg_cut.c m_Rndcg.c m_ndcg_rel.c m_binG.c m_G.c m_rel_P.c m_success.c m_infap.c m_map_cut.c m_gm_bpref.c m_runid.c m_relstring.c m_set_P.c m_set_recall.c m_set_rel_P.c m_set_map.c m_set_F.c m_num_nonrel_judged_ret.c m_prefs_num_prefs_poss.c m_prefs_num_prefs_ful.c m_prefs_num_prefs_ful_ret.c m_prefs_simp.c m_prefs_pair.c m_prefs_avgjg.c m_prefs_avgjg_Rnonrel.c m_prefs_simp_ret.c m_prefs_pair_ret.c m_prefs_avgjg_ret.c m_prefs_avgjg_Rnonrel_ret.c m_prefs_simp_imp.c m_prefs_pair_imp.c m_prefs_avgjg_imp.c m_map_avgjg.c m_Rprec_mult_avgjg.c m_P_avgjg.c m_yaap.c -lm\n",
            "/content/TREC_COVID_sentence_transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eph6kJ0Z8iu0"
      },
      "source": [
        "All official metrics.\r\n",
        "Remember to change the `.txt` file name with the one you want to evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVJqqruwvV4J",
        "outputId": "ad5ca253-e75e-463b-e3f4-bda601f06156"
      },
      "source": [
        "!./trec_eval/trec_eval ./trec_covid_data/qrels-rnd1.txt ./results/ensemble_3_models.txt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "runid                 \tall\tTFM\n",
            "num_q                 \tall\t30\n",
            "num_ret               \tall\t30000\n",
            "num_rel               \tall\t2352\n",
            "num_rel_ret           \tall\t1160\n",
            "map                   \tall\t0.1893\n",
            "gm_map                \tall\t0.1307\n",
            "Rprec                 \tall\t0.2497\n",
            "bpref                 \tall\t0.3588\n",
            "recip_rank            \tall\t0.8378\n",
            "iprec_at_recall_0.00  \tall\t0.8601\n",
            "iprec_at_recall_0.10  \tall\t0.5361\n",
            "iprec_at_recall_0.20  \tall\t0.3905\n",
            "iprec_at_recall_0.30  \tall\t0.2651\n",
            "iprec_at_recall_0.40  \tall\t0.1730\n",
            "iprec_at_recall_0.50  \tall\t0.1067\n",
            "iprec_at_recall_0.60  \tall\t0.0427\n",
            "iprec_at_recall_0.70  \tall\t0.0182\n",
            "iprec_at_recall_0.80  \tall\t0.0031\n",
            "iprec_at_recall_0.90  \tall\t0.0000\n",
            "iprec_at_recall_1.00  \tall\t0.0000\n",
            "P_5                   \tall\t0.6333\n",
            "P_10                  \tall\t0.5367\n",
            "P_15                  \tall\t0.4778\n",
            "P_20                  \tall\t0.4450\n",
            "P_30                  \tall\t0.3822\n",
            "P_100                 \tall\t0.2050\n",
            "P_200                 \tall\t0.1300\n",
            "P_500                 \tall\t0.0649\n",
            "P_1000                \tall\t0.0387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIVm-7vY8mOm"
      },
      "source": [
        "ndcg metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ4zQ6SM3Z2h",
        "outputId": "2e206a9d-9242-4312-f8b6-204ef242eb4c"
      },
      "source": [
        "!./trec_eval/trec_eval -m ndcg_cut ./trec_covid_data/qrels-rnd1.txt ./results/ensemble_3_models.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ndcg_cut_5            \tall\t0.6059\n",
            "ndcg_cut_10           \tall\t0.5297\n",
            "ndcg_cut_15           \tall\t0.4820\n",
            "ndcg_cut_20           \tall\t0.4542\n",
            "ndcg_cut_30           \tall\t0.4096\n",
            "ndcg_cut_100          \tall\t0.3538\n",
            "ndcg_cut_200          \tall\t0.3820\n",
            "ndcg_cut_500          \tall\t0.4207\n",
            "ndcg_cut_1000         \tall\t0.4547\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}